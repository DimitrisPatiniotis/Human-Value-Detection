{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLB3I4FKZ5Lr"
   },
   "source": [
    "# Fine-tuning BERT (and friends) for multi-label text classification\n",
    "\n",
    "In this notebook, we are going to fine-tune BERT to predict one or more labels for a given piece of text. Note that this notebook illustrates how to fine-tune a bert-base-uncased model, but you can also fine-tune a RoBERTa, DeBERTa, DistilBERT, CANINE, ... checkpoint in the same way. \n",
    "\n",
    "All of those work in the same way: they add a linear layer on top of the base model, which is used to produce a tensor of shape (batch_size, num_labels), indicating the unnormalized scores for a number of labels for every example in the batch.\n",
    "\n",
    "\n",
    "\n",
    "## Set-up environment\n",
    "\n",
    "First, we install the libraries which we'll use: HuggingFace Transformers and Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4wxY3x-ZZz8h"
   },
   "outputs": [],
   "source": [
    "#!pip install -q transformers datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIH9NP0MZ6-O"
   },
   "source": [
    "## Load dataset\n",
    "\n",
    "Next, let's download a multi-label text classification dataset from the [hub](https://huggingface.co/).\n",
    "\n",
    "At the time of writing, I picked a random one as follows:   \n",
    "\n",
    "* first, go to the \"datasets\" tab on huggingface.co\n",
    "* next, select the \"multi-label-classification\" tag on the left as well as the the \"1k<10k\" tag (fo find a relatively small dataset).\n",
    "\n",
    "Note that you can also easily load your local data (i.e. csv files, txt files, Parquet files, JSON, ...) as explained [here](https://huggingface.co/docs/datasets/loading.html#local-and-remote-files).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "seed = 2022\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "  torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86,
     "referenced_widgets": [
      "9a1aa9f2cc29473f9f8e5459d2641e76",
      "308fa6a7348140ec981a8d6c7d31f346",
      "8bc92587e35443488445e7521fbd0a13",
      "091f8220f33241f288faa0612853585f",
      "1045bb16e3694410898a73cf1b848917",
      "cb95e545fbdd4e99903bf634df694c9f",
      "5080d322a8034924b652b379c04667ed",
      "8b1899a0c4b144d7a5e6599f8afb8b65",
      "6111a73e684a47769bda7183a836ee91",
      "cd3570ddf67541d7818d97e236c54e54",
      "bbba60f793c14100934a268063f63d26"
     ]
    },
    "id": "sd1LiXGjZ420",
    "outputId": "1b5783cd-0e4e-4c92-c67c-57b9288f2381"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_args = pd.read_csv('../../../Data/arguments-training.tsv',sep = '\\t')\n",
    "df_lbls = pd.read_table('../../../Data/labels-training.tsv')\n",
    "df = df_args.merge(df_lbls, how=\"left\", on=\"Argument ID\")\n",
    "\n",
    "# print(df)\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "train_dataset = Dataset.from_pandas(df, split=\"train\")\n",
    "dataset = DatasetDict({ \"train\": train_dataset, \"validation\": train_dataset, \"test\": train_dataset })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCL02vQgxYTO"
   },
   "source": [
    "As we can see, the dataset contains 3 splits: one for training, one for validation and one for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pRd1kXQZjYIY",
    "outputId": "8021590c-5607-4a9c-a474-bc57da503c93"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Argument ID', 'Conclusion', 'Stance', 'Premise', 'Self-direction: thought', 'Self-direction: action', 'Stimulation', 'Hedonism', 'Achievement', 'Power: dominance', 'Power: resources', 'Face', 'Security: personal', 'Security: societal', 'Tradition', 'Conformity: rules', 'Conformity: interpersonal', 'Humility', 'Benevolence: caring', 'Benevolence: dependability', 'Universalism: concern', 'Universalism: nature', 'Universalism: tolerance', 'Universalism: objectivity', '__index_level_0__'],\n",
       "        num_rows: 5220\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Argument ID', 'Conclusion', 'Stance', 'Premise', 'Self-direction: thought', 'Self-direction: action', 'Stimulation', 'Hedonism', 'Achievement', 'Power: dominance', 'Power: resources', 'Face', 'Security: personal', 'Security: societal', 'Tradition', 'Conformity: rules', 'Conformity: interpersonal', 'Humility', 'Benevolence: caring', 'Benevolence: dependability', 'Universalism: concern', 'Universalism: nature', 'Universalism: tolerance', 'Universalism: objectivity', '__index_level_0__'],\n",
       "        num_rows: 5220\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Argument ID', 'Conclusion', 'Stance', 'Premise', 'Self-direction: thought', 'Self-direction: action', 'Stimulation', 'Hedonism', 'Achievement', 'Power: dominance', 'Power: resources', 'Face', 'Security: personal', 'Security: societal', 'Tradition', 'Conformity: rules', 'Conformity: interpersonal', 'Humility', 'Benevolence: caring', 'Benevolence: dependability', 'Universalism: concern', 'Universalism: nature', 'Universalism: tolerance', 'Universalism: objectivity', '__index_level_0__'],\n",
       "        num_rows: 5220\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgS0wMWExcqP"
   },
   "source": [
    "Let's check the first example of the training split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "unjuTtKUjZI3",
    "outputId": "6f1e5051-8272-40f9-ced8-ba17a105e904"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Argument ID': 'A01001',\n",
       " 'Conclusion': 'Entrapment should be legalized',\n",
       " 'Stance': 'in favor of',\n",
       " 'Premise': \"if entrapment can serve to more easily capture wanted criminals, then why shouldn't it be legal?\",\n",
       " 'Self-direction: thought': 0,\n",
       " 'Self-direction: action': 0,\n",
       " 'Stimulation': 0,\n",
       " 'Hedonism': 0,\n",
       " 'Achievement': 0,\n",
       " 'Power: dominance': 0,\n",
       " 'Power: resources': 0,\n",
       " 'Face': 0,\n",
       " 'Security: personal': 0,\n",
       " 'Security: societal': 1,\n",
       " 'Tradition': 0,\n",
       " 'Conformity: rules': 0,\n",
       " 'Conformity: interpersonal': 0,\n",
       " 'Humility': 0,\n",
       " 'Benevolence: caring': 0,\n",
       " 'Benevolence: dependability': 0,\n",
       " 'Universalism: concern': 0,\n",
       " 'Universalism: nature': 0,\n",
       " 'Universalism: tolerance': 0,\n",
       " 'Universalism: objectivity': 0,\n",
       " '__index_level_0__': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = dataset['train'][0]\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DV0Rtetxgd4"
   },
   "source": [
    "The dataset consists of arguments, labeled with one or more values. \n",
    "\n",
    "Let's create a list that contains the labels, as well as 2 dictionaries that map labels to integers and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e5vZhQpvkE8s",
    "outputId": "5d513b30-f209-492f-c6ab-245d64a67d40"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Self-direction: thought',\n",
       " 'Self-direction: action',\n",
       " 'Stimulation',\n",
       " 'Hedonism',\n",
       " 'Achievement',\n",
       " 'Power: dominance',\n",
       " 'Power: resources',\n",
       " 'Face',\n",
       " 'Security: personal',\n",
       " 'Security: societal',\n",
       " 'Tradition',\n",
       " 'Conformity: rules',\n",
       " 'Conformity: interpersonal',\n",
       " 'Humility',\n",
       " 'Benevolence: caring',\n",
       " 'Benevolence: dependability',\n",
       " 'Universalism: concern',\n",
       " 'Universalism: nature',\n",
       " 'Universalism: tolerance',\n",
       " 'Universalism: objectivity']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [label for label in dataset['train'].features.keys() if label not in ['Argument ID', 'Conclusion', 'Stance', 'Premise', '__index_level_0__']]\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJ3Teyjmank2"
   },
   "source": [
    "## Preprocess data\n",
    "\n",
    "As models like BERT don't expect text as direct input, but rather `input_ids`, etc., we tokenize the text using the tokenizer. Here I'm using the `AutoTokenizer` API, which will automatically load the appropriate tokenizer based on the checkpoint on the hub.\n",
    "\n",
    "What's a bit tricky is that we also need to provide labels to the model. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). Also important: this should be a tensor of floats rather than integers, otherwise PyTorch' `BCEWithLogitsLoss` (which the model will use) will complain, as explained [here](https://discuss.pytorch.org/t/multi-label-binary-classification-result-type-float-cant-be-cast-to-the-desired-output-type-long/117915/3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "AFWlSsbZaRLc"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def preprocess_data(examples):\n",
    "  # take a batch of texts\n",
    "  premise    = examples[\"Premise\"]\n",
    "  conclusion = examples[\"Conclusion\"]\n",
    "  # encode them\n",
    "  encoding = tokenizer(premise, conclusion, padding=\"max_length\", truncation=True, max_length=128)\n",
    "  # add labels\n",
    "  labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n",
    "  # create numpy array of shape (batch_size, num_labels)\n",
    "  labels_matrix = np.zeros((len(premise), len(labels)))\n",
    "  # fill numpy array\n",
    "  for idx, label in enumerate(labels):\n",
    "    labels_matrix[:, idx] = labels_batch[label]\n",
    "\n",
    "  encoding[\"labels\"] = labels_matrix.tolist()\n",
    "  \n",
    "  return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i4ENBTdulBEI",
    "outputId": "02554a1f-4961-461a-bf29-555b8debeabf"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89745e2a5aa141b4a80333fb570eda63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04f84e47400046ab82fd3fca74b1302a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6305ea59cc1044528bff3c68c8fa14f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_dataset = dataset.map(preprocess_data, batched=True, remove_columns=dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0enAb0W9o25W",
    "outputId": "55bc5ba6-d169-49c6-f562-bb7ea4143866"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "example = encoded_dataset['train'][0]\n",
    "print(example.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "id": "D0McCtJ8HRJY",
    "outputId": "82fb0336-51a3-40ad-ebc0-65eeb7cf4b6c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] if entrapment can serve to more easily capture wanted criminals, then why shouldn't it be legal? [SEP] entrapment should be legalized [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(example['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VdIvj6WjHeZQ",
    "outputId": "418c14d2-cca3-44e9-d0a2-6ad4ca7a007c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q4Dx95t2o6N9",
    "outputId": "3ce6c923-0b45-4743-bc4b-7771d088b03e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Security: societal']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[id2label[idx] for idx, label in enumerate(example['labels']) if label == 1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-fold cross validation\n",
    "Prepare the folds, for n-fold cross validation.\n",
    "We want to use stratification, although we have a multi-label classification problem, which StratifiedKFold cannot handle. (You get an error: \"ValueError: Supported target types are: ('binary', 'multiclass'). Got 'multilabel-indicator' instead.\")\n",
    "\n",
    "A solution is to convert from multi-label to multi-class. Assuming you have n classes and the target variable is a combination of these n classes. We will have (2^n) - 1 combinations (Not including all 0s).\n",
    "\n",
    "Lets create a new target variable considering each combination as a new label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def multilabel_to_string(y):\n",
    "    return '-'.join(str(int(l.item())) for l in y)\n",
    "\n",
    "def multilabel_to_multiclass(y):\n",
    "    y_new = LabelEncoder().fit_transform([multilabel_to_string(l) for l in y])\n",
    "    return y_new\n",
    "\n",
    "# multilabel_to_multiclass(encoded_dataset[\"train\"][\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HgpKXDfvKBxn"
   },
   "source": [
    "Finally, we set the format of our data to PyTorch tensors. This will turn the training, validation and test sets into standard PyTorch [datasets](https://pytorch.org/docs/stable/data.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Lk6Cq9duKBkA"
   },
   "outputs": [],
   "source": [
    "encoded_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "n_folds = 5\n",
    "\n",
    "# First make the kfold object\n",
    "folds = StratifiedKFold(n_splits=n_folds)\n",
    "\n",
    "# Now make our splits based off of the labels. \n",
    "# We can use `np.zeros()` here since it only works off of indices, we really care about the labels\n",
    "splits = folds.split(np.zeros(dataset[\"train\"].num_rows), multilabel_to_multiclass(encoded_dataset[\"train\"][\"labels\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5qSmCgWefWs"
   },
   "source": [
    "## Define model\n",
    "\n",
    "Here we define a model that includes a pre-trained base (i.e. the weights from bert-base-uncased) are loaded, with a random initialized classification head (linear layer) on top. One should fine-tune this head, together with the pre-trained base on a labeled dataset.\n",
    "\n",
    "This is also printed by the warning.\n",
    "\n",
    "We set the `problem_type` to be \"multi_label_classification\", as this will make sure the appropriate loss function is used (namely [`BCEWithLogitsLoss`](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)). We also make sure the output layer has `len(labels)` output neurons, and we set the id2label and label2id mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6XPL1Z_RegBF",
    "outputId": "22994300-8c93-421e-faa4-678d6cc14aab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(labels),\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjJGEXShp7te"
   },
   "source": [
    "## Train the model!\n",
    "\n",
    "We are going to train the model using HuggingFace's Trainer API. This requires us to define 2 things: \n",
    "\n",
    "* `TrainingArguments`, which specify training hyperparameters. All options can be found in the [docs](https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments). Below, we for example specify that we want to evaluate after every epoch of training, we would like to save the model every epoch, we set the learning rate, the batch size to use for training/evaluation, how many epochs to train for, and so on.\n",
    "* a `Trainer` object (docs can be found [here](https://huggingface.co/transformers/main_classes/trainer.html#id1))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "K5a8_vIKqr7P"
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "metric_name = \"f1\"\n",
    "num_train_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "dR2GmpvDqbuZ"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"bert-finetuned-sem_eval-english\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    #push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_v2fPFFJ3-v"
   },
   "source": [
    "We are also going to compute metrics while training. For this, we need to define a `compute_metrics` function, that returns a dictionary with the desired metric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "797b2WHJqUgZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "    \n",
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    precision_micro_average = precision_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    recall_micro_average = recall_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'p': precision_micro_average,\n",
    "               'r': recall_micro_average,\n",
    "               'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxNo4_TsvzDm"
   },
   "source": [
    "Let's verify a batch as well as a forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "IlOgGiojuWwG",
    "outputId": "cd0b2c99-b520-468d-8ffc-c36211b7820a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset['train'][0]['labels'].type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y41Kre_jvD7x",
    "outputId": "b6ca888b-6371-40fb-ab83-3dc24d28320a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  2065,  4372,  6494, 24073,  2064,  3710,  2000,  2062,  4089,\n",
       "         5425,  2359, 12290,  1010,  2059,  2339,  5807,  1005,  1056,  2009,\n",
       "         2022,  3423,  1029,   102,  4372,  6494, 24073,  2323,  2022,  3423,\n",
       "         3550,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset['train']['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sxWcnZ8ku12V",
    "outputId": "26522911-c3cd-466a-ae2d-d81d23003c23"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.6951, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), logits=tensor([[-0.0848, -0.4281,  0.5540,  0.1601, -0.0590,  0.0067, -0.0180, -0.4107,\n",
       "         -0.4852,  0.0186, -0.2489, -0.7731,  0.0675,  0.1176,  0.2130,  0.0916,\n",
       "         -0.0998,  0.2706,  0.8651, -0.2715]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#forward pass\n",
    "outputs = model(input_ids=encoded_dataset['train']['input_ids'][0].unsqueeze(0), labels=encoded_dataset['train'][0]['labels'].unsqueeze(0))\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-X2brZcv0X6"
   },
   "source": [
    "Let's start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "chq_3nUz73ib",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.10/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/petasis/.local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4176\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5220\n",
      "  Number of trainable parameters = 109497620\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5220' max='5220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5220/5220 15:34, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>P</th>\n",
       "      <th>R</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.409900</td>\n",
       "      <td>0.357382</td>\n",
       "      <td>0.705655</td>\n",
       "      <td>0.256764</td>\n",
       "      <td>0.376524</td>\n",
       "      <td>0.617420</td>\n",
       "      <td>0.052682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.339900</td>\n",
       "      <td>0.334192</td>\n",
       "      <td>0.692629</td>\n",
       "      <td>0.373450</td>\n",
       "      <td>0.485259</td>\n",
       "      <td>0.669762</td>\n",
       "      <td>0.076628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.299500</td>\n",
       "      <td>0.325797</td>\n",
       "      <td>0.686616</td>\n",
       "      <td>0.410654</td>\n",
       "      <td>0.513933</td>\n",
       "      <td>0.686143</td>\n",
       "      <td>0.069923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.266900</td>\n",
       "      <td>0.322368</td>\n",
       "      <td>0.697862</td>\n",
       "      <td>0.414036</td>\n",
       "      <td>0.519724</td>\n",
       "      <td>0.688670</td>\n",
       "      <td>0.072797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.238400</td>\n",
       "      <td>0.325592</td>\n",
       "      <td>0.677826</td>\n",
       "      <td>0.439402</td>\n",
       "      <td>0.533174</td>\n",
       "      <td>0.698325</td>\n",
       "      <td>0.070881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.213100</td>\n",
       "      <td>0.330223</td>\n",
       "      <td>0.675011</td>\n",
       "      <td>0.448422</td>\n",
       "      <td>0.538865</td>\n",
       "      <td>0.702113</td>\n",
       "      <td>0.075670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.192600</td>\n",
       "      <td>0.336274</td>\n",
       "      <td>0.654187</td>\n",
       "      <td>0.458005</td>\n",
       "      <td>0.538793</td>\n",
       "      <td>0.704222</td>\n",
       "      <td>0.063218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.178700</td>\n",
       "      <td>0.340527</td>\n",
       "      <td>0.659901</td>\n",
       "      <td>0.448985</td>\n",
       "      <td>0.534384</td>\n",
       "      <td>0.700808</td>\n",
       "      <td>0.068008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.166000</td>\n",
       "      <td>0.344301</td>\n",
       "      <td>0.648478</td>\n",
       "      <td>0.462232</td>\n",
       "      <td>0.539740</td>\n",
       "      <td>0.705470</td>\n",
       "      <td>0.060345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.156400</td>\n",
       "      <td>0.346513</td>\n",
       "      <td>0.644783</td>\n",
       "      <td>0.465051</td>\n",
       "      <td>0.540364</td>\n",
       "      <td>0.706302</td>\n",
       "      <td>0.058429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-522\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-522/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-522/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-522/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-522/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-1044\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-1044/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-1044/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-1044/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-1044/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-1566\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-1566/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-1566/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-1566/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-1566/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-2088\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-2088/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-2088/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-2088/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-2088/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-2610\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-2610/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-2610/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-2610/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-2610/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-3132\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-3132/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-3132/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-3132/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-3132/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-3654\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-3654/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-3654/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-3654/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-3654/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-4176\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-4176/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-4176/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-4176/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-4176/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-4698\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-4698/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-4698/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-4698/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-4698/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-5220\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-5220/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-5220/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-5220/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-5220/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from bert-finetuned-sem_eval-english/checkpoint-5220 (score: 0.5403635172752579).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='131' max='131' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [131/131 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/petasis/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Self-direction: thought\",\n",
      "    \"1\": \"Self-direction: action\",\n",
      "    \"2\": \"Stimulation\",\n",
      "    \"3\": \"Hedonism\",\n",
      "    \"4\": \"Achievement\",\n",
      "    \"5\": \"Power: dominance\",\n",
      "    \"6\": \"Power: resources\",\n",
      "    \"7\": \"Face\",\n",
      "    \"8\": \"Security: personal\",\n",
      "    \"9\": \"Security: societal\",\n",
      "    \"10\": \"Tradition\",\n",
      "    \"11\": \"Conformity: rules\",\n",
      "    \"12\": \"Conformity: interpersonal\",\n",
      "    \"13\": \"Humility\",\n",
      "    \"14\": \"Benevolence: caring\",\n",
      "    \"15\": \"Benevolence: dependability\",\n",
      "    \"16\": \"Universalism: concern\",\n",
      "    \"17\": \"Universalism: nature\",\n",
      "    \"18\": \"Universalism: tolerance\",\n",
      "    \"19\": \"Universalism: objectivity\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Achievement\": 4,\n",
      "    \"Benevolence: caring\": 14,\n",
      "    \"Benevolence: dependability\": 15,\n",
      "    \"Conformity: interpersonal\": 12,\n",
      "    \"Conformity: rules\": 11,\n",
      "    \"Face\": 7,\n",
      "    \"Hedonism\": 3,\n",
      "    \"Humility\": 13,\n",
      "    \"Power: dominance\": 5,\n",
      "    \"Power: resources\": 6,\n",
      "    \"Security: personal\": 8,\n",
      "    \"Security: societal\": 9,\n",
      "    \"Self-direction: action\": 1,\n",
      "    \"Self-direction: thought\": 0,\n",
      "    \"Stimulation\": 2,\n",
      "    \"Tradition\": 10,\n",
      "    \"Universalism: concern\": 16,\n",
      "    \"Universalism: nature\": 17,\n",
      "    \"Universalism: objectivity\": 19,\n",
      "    \"Universalism: tolerance\": 18\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/petasis/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/petasis/.local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4176\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5220\n",
      "  Number of trainable parameters = 109497620\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5220' max='5220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5220/5220 15:07, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>P</th>\n",
       "      <th>R</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.413800</td>\n",
       "      <td>0.360070</td>\n",
       "      <td>0.723324</td>\n",
       "      <td>0.214746</td>\n",
       "      <td>0.331172</td>\n",
       "      <td>0.598911</td>\n",
       "      <td>0.040230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.344000</td>\n",
       "      <td>0.329879</td>\n",
       "      <td>0.744853</td>\n",
       "      <td>0.334735</td>\n",
       "      <td>0.461896</td>\n",
       "      <td>0.655556</td>\n",
       "      <td>0.071839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.306800</td>\n",
       "      <td>0.319158</td>\n",
       "      <td>0.741407</td>\n",
       "      <td>0.393047</td>\n",
       "      <td>0.513741</td>\n",
       "      <td>0.682401</td>\n",
       "      <td>0.091954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.271700</td>\n",
       "      <td>0.316813</td>\n",
       "      <td>0.707891</td>\n",
       "      <td>0.430053</td>\n",
       "      <td>0.535054</td>\n",
       "      <td>0.696746</td>\n",
       "      <td>0.090996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.243000</td>\n",
       "      <td>0.316959</td>\n",
       "      <td>0.695466</td>\n",
       "      <td>0.455845</td>\n",
       "      <td>0.550720</td>\n",
       "      <td>0.707360</td>\n",
       "      <td>0.094828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.216200</td>\n",
       "      <td>0.321696</td>\n",
       "      <td>0.677514</td>\n",
       "      <td>0.455285</td>\n",
       "      <td>0.544601</td>\n",
       "      <td>0.705318</td>\n",
       "      <td>0.086207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.196800</td>\n",
       "      <td>0.328105</td>\n",
       "      <td>0.668462</td>\n",
       "      <td>0.452201</td>\n",
       "      <td>0.539465</td>\n",
       "      <td>0.702996</td>\n",
       "      <td>0.087165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.183600</td>\n",
       "      <td>0.330659</td>\n",
       "      <td>0.654905</td>\n",
       "      <td>0.473507</td>\n",
       "      <td>0.549626</td>\n",
       "      <td>0.711050</td>\n",
       "      <td>0.086207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.168100</td>\n",
       "      <td>0.333355</td>\n",
       "      <td>0.658721</td>\n",
       "      <td>0.464816</td>\n",
       "      <td>0.545036</td>\n",
       "      <td>0.707600</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.161600</td>\n",
       "      <td>0.334438</td>\n",
       "      <td>0.655952</td>\n",
       "      <td>0.463415</td>\n",
       "      <td>0.543125</td>\n",
       "      <td>0.706668</td>\n",
       "      <td>0.080460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-522\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-522/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-522/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-522/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-522/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-1044\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-1044/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-1044/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-1044/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-1044/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-1566\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-1566/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-1566/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-1566/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-1566/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-2088\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-2088/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-2088/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-2088/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-2088/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-2610\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-2610/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-2610/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-2610/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-2610/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-3132\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-3132/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-3132/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-3132/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-3132/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-3654\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-3654/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-3654/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-3654/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-3654/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-4176\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-4176/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-4176/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-4176/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-4176/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-4698\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-4698/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-4698/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-4698/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-4698/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-5220\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-5220/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-5220/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-5220/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-5220/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from bert-finetuned-sem_eval-english/checkpoint-2610 (score: 0.5507197290431837).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='131' max='131' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [131/131 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/petasis/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Self-direction: thought\",\n",
      "    \"1\": \"Self-direction: action\",\n",
      "    \"2\": \"Stimulation\",\n",
      "    \"3\": \"Hedonism\",\n",
      "    \"4\": \"Achievement\",\n",
      "    \"5\": \"Power: dominance\",\n",
      "    \"6\": \"Power: resources\",\n",
      "    \"7\": \"Face\",\n",
      "    \"8\": \"Security: personal\",\n",
      "    \"9\": \"Security: societal\",\n",
      "    \"10\": \"Tradition\",\n",
      "    \"11\": \"Conformity: rules\",\n",
      "    \"12\": \"Conformity: interpersonal\",\n",
      "    \"13\": \"Humility\",\n",
      "    \"14\": \"Benevolence: caring\",\n",
      "    \"15\": \"Benevolence: dependability\",\n",
      "    \"16\": \"Universalism: concern\",\n",
      "    \"17\": \"Universalism: nature\",\n",
      "    \"18\": \"Universalism: tolerance\",\n",
      "    \"19\": \"Universalism: objectivity\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Achievement\": 4,\n",
      "    \"Benevolence: caring\": 14,\n",
      "    \"Benevolence: dependability\": 15,\n",
      "    \"Conformity: interpersonal\": 12,\n",
      "    \"Conformity: rules\": 11,\n",
      "    \"Face\": 7,\n",
      "    \"Hedonism\": 3,\n",
      "    \"Humility\": 13,\n",
      "    \"Power: dominance\": 5,\n",
      "    \"Power: resources\": 6,\n",
      "    \"Security: personal\": 8,\n",
      "    \"Security: societal\": 9,\n",
      "    \"Self-direction: action\": 1,\n",
      "    \"Self-direction: thought\": 0,\n",
      "    \"Stimulation\": 2,\n",
      "    \"Tradition\": 10,\n",
      "    \"Universalism: concern\": 16,\n",
      "    \"Universalism: nature\": 17,\n",
      "    \"Universalism: objectivity\": 19,\n",
      "    \"Universalism: tolerance\": 18\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/petasis/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/petasis/.local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4176\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5220\n",
      "  Number of trainable parameters = 109497620\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5220' max='5220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5220/5220 15:10, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>P</th>\n",
       "      <th>R</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.413800</td>\n",
       "      <td>0.364698</td>\n",
       "      <td>0.707972</td>\n",
       "      <td>0.249722</td>\n",
       "      <td>0.369212</td>\n",
       "      <td>0.614160</td>\n",
       "      <td>0.045019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.345900</td>\n",
       "      <td>0.338041</td>\n",
       "      <td>0.720742</td>\n",
       "      <td>0.346325</td>\n",
       "      <td>0.467845</td>\n",
       "      <td>0.659222</td>\n",
       "      <td>0.074713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.309200</td>\n",
       "      <td>0.325164</td>\n",
       "      <td>0.716319</td>\n",
       "      <td>0.392261</td>\n",
       "      <td>0.506926</td>\n",
       "      <td>0.679992</td>\n",
       "      <td>0.089080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.275800</td>\n",
       "      <td>0.317130</td>\n",
       "      <td>0.707006</td>\n",
       "      <td>0.432628</td>\n",
       "      <td>0.536788</td>\n",
       "      <td>0.697688</td>\n",
       "      <td>0.090996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.246700</td>\n",
       "      <td>0.321620</td>\n",
       "      <td>0.705244</td>\n",
       "      <td>0.434298</td>\n",
       "      <td>0.537560</td>\n",
       "      <td>0.698292</td>\n",
       "      <td>0.092912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.220300</td>\n",
       "      <td>0.323508</td>\n",
       "      <td>0.687092</td>\n",
       "      <td>0.468263</td>\n",
       "      <td>0.556954</td>\n",
       "      <td>0.711977</td>\n",
       "      <td>0.092912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.199200</td>\n",
       "      <td>0.327644</td>\n",
       "      <td>0.669981</td>\n",
       "      <td>0.487751</td>\n",
       "      <td>0.564524</td>\n",
       "      <td>0.718916</td>\n",
       "      <td>0.091954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.184100</td>\n",
       "      <td>0.333248</td>\n",
       "      <td>0.680209</td>\n",
       "      <td>0.470768</td>\n",
       "      <td>0.556433</td>\n",
       "      <td>0.712391</td>\n",
       "      <td>0.089080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.173500</td>\n",
       "      <td>0.333745</td>\n",
       "      <td>0.676743</td>\n",
       "      <td>0.467428</td>\n",
       "      <td>0.552939</td>\n",
       "      <td>0.710519</td>\n",
       "      <td>0.095785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.164600</td>\n",
       "      <td>0.334893</td>\n",
       "      <td>0.677602</td>\n",
       "      <td>0.465757</td>\n",
       "      <td>0.552054</td>\n",
       "      <td>0.709857</td>\n",
       "      <td>0.093870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-522\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-522/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-522/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-522/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-522/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-1044\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-1044/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-1044/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-1044/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-1044/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-1566\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-1566/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-1566/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-1566/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-1566/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-2088\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-2088/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-2088/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-2088/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-2088/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-2610\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-2610/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-2610/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-2610/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-2610/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-3132\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-3132/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-3132/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-3132/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-3132/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-3654\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-3654/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-3654/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-3654/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-3654/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-4176\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-4176/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-4176/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-4176/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-4176/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-4698\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-4698/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-4698/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-4698/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-4698/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-5220\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-5220/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-5220/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-5220/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-5220/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from bert-finetuned-sem_eval-english/checkpoint-3654 (score: 0.5645239246012567).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='131' max='131' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [131/131 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/petasis/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Self-direction: thought\",\n",
      "    \"1\": \"Self-direction: action\",\n",
      "    \"2\": \"Stimulation\",\n",
      "    \"3\": \"Hedonism\",\n",
      "    \"4\": \"Achievement\",\n",
      "    \"5\": \"Power: dominance\",\n",
      "    \"6\": \"Power: resources\",\n",
      "    \"7\": \"Face\",\n",
      "    \"8\": \"Security: personal\",\n",
      "    \"9\": \"Security: societal\",\n",
      "    \"10\": \"Tradition\",\n",
      "    \"11\": \"Conformity: rules\",\n",
      "    \"12\": \"Conformity: interpersonal\",\n",
      "    \"13\": \"Humility\",\n",
      "    \"14\": \"Benevolence: caring\",\n",
      "    \"15\": \"Benevolence: dependability\",\n",
      "    \"16\": \"Universalism: concern\",\n",
      "    \"17\": \"Universalism: nature\",\n",
      "    \"18\": \"Universalism: tolerance\",\n",
      "    \"19\": \"Universalism: objectivity\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Achievement\": 4,\n",
      "    \"Benevolence: caring\": 14,\n",
      "    \"Benevolence: dependability\": 15,\n",
      "    \"Conformity: interpersonal\": 12,\n",
      "    \"Conformity: rules\": 11,\n",
      "    \"Face\": 7,\n",
      "    \"Hedonism\": 3,\n",
      "    \"Humility\": 13,\n",
      "    \"Power: dominance\": 5,\n",
      "    \"Power: resources\": 6,\n",
      "    \"Security: personal\": 8,\n",
      "    \"Security: societal\": 9,\n",
      "    \"Self-direction: action\": 1,\n",
      "    \"Self-direction: thought\": 0,\n",
      "    \"Stimulation\": 2,\n",
      "    \"Tradition\": 10,\n",
      "    \"Universalism: concern\": 16,\n",
      "    \"Universalism: nature\": 17,\n",
      "    \"Universalism: objectivity\": 19,\n",
      "    \"Universalism: tolerance\": 18\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/petasis/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/petasis/.local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4176\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5220\n",
      "  Number of trainable parameters = 109497620\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5220' max='5220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5220/5220 14:54, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>P</th>\n",
       "      <th>R</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.412800</td>\n",
       "      <td>0.362643</td>\n",
       "      <td>0.713829</td>\n",
       "      <td>0.248124</td>\n",
       "      <td>0.368247</td>\n",
       "      <td>0.613704</td>\n",
       "      <td>0.041188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.341300</td>\n",
       "      <td>0.334999</td>\n",
       "      <td>0.711965</td>\n",
       "      <td>0.365379</td>\n",
       "      <td>0.482923</td>\n",
       "      <td>0.667297</td>\n",
       "      <td>0.084291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.304200</td>\n",
       "      <td>0.325017</td>\n",
       "      <td>0.724645</td>\n",
       "      <td>0.383162</td>\n",
       "      <td>0.501272</td>\n",
       "      <td>0.676420</td>\n",
       "      <td>0.091954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.269800</td>\n",
       "      <td>0.322860</td>\n",
       "      <td>0.714216</td>\n",
       "      <td>0.409003</td>\n",
       "      <td>0.520141</td>\n",
       "      <td>0.687459</td>\n",
       "      <td>0.096743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.239000</td>\n",
       "      <td>0.324058</td>\n",
       "      <td>0.685903</td>\n",
       "      <td>0.432620</td>\n",
       "      <td>0.530584</td>\n",
       "      <td>0.695680</td>\n",
       "      <td>0.090996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.214300</td>\n",
       "      <td>0.330634</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.437066</td>\n",
       "      <td>0.534398</td>\n",
       "      <td>0.697845</td>\n",
       "      <td>0.096743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.193800</td>\n",
       "      <td>0.333777</td>\n",
       "      <td>0.677145</td>\n",
       "      <td>0.442901</td>\n",
       "      <td>0.535528</td>\n",
       "      <td>0.699461</td>\n",
       "      <td>0.105364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.177300</td>\n",
       "      <td>0.339131</td>\n",
       "      <td>0.679449</td>\n",
       "      <td>0.438177</td>\n",
       "      <td>0.532770</td>\n",
       "      <td>0.697562</td>\n",
       "      <td>0.106322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.168200</td>\n",
       "      <td>0.343442</td>\n",
       "      <td>0.675364</td>\n",
       "      <td>0.438733</td>\n",
       "      <td>0.531918</td>\n",
       "      <td>0.697406</td>\n",
       "      <td>0.102490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.158800</td>\n",
       "      <td>0.344241</td>\n",
       "      <td>0.672964</td>\n",
       "      <td>0.445402</td>\n",
       "      <td>0.536031</td>\n",
       "      <td>0.700162</td>\n",
       "      <td>0.105364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-522\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-522/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-522/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-522/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-522/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-1044\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-1044/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-1044/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-1044/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-1044/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-1566\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-1566/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-1566/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-1566/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-1566/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-2088\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-2088/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-2088/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-2088/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-2088/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-2610\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-2610/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-2610/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-2610/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-2610/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-3132\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-3132/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-3132/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-3132/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-3132/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-3654\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-3654/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-3654/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-3654/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-3654/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-4176\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-4176/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-4176/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-4176/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-4176/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-4698\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-4698/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-4698/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-4698/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-4698/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-5220\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-5220/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-5220/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-5220/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-5220/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from bert-finetuned-sem_eval-english/checkpoint-5220 (score: 0.5360307640862733).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='131' max='131' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [131/131 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/petasis/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Self-direction: thought\",\n",
      "    \"1\": \"Self-direction: action\",\n",
      "    \"2\": \"Stimulation\",\n",
      "    \"3\": \"Hedonism\",\n",
      "    \"4\": \"Achievement\",\n",
      "    \"5\": \"Power: dominance\",\n",
      "    \"6\": \"Power: resources\",\n",
      "    \"7\": \"Face\",\n",
      "    \"8\": \"Security: personal\",\n",
      "    \"9\": \"Security: societal\",\n",
      "    \"10\": \"Tradition\",\n",
      "    \"11\": \"Conformity: rules\",\n",
      "    \"12\": \"Conformity: interpersonal\",\n",
      "    \"13\": \"Humility\",\n",
      "    \"14\": \"Benevolence: caring\",\n",
      "    \"15\": \"Benevolence: dependability\",\n",
      "    \"16\": \"Universalism: concern\",\n",
      "    \"17\": \"Universalism: nature\",\n",
      "    \"18\": \"Universalism: tolerance\",\n",
      "    \"19\": \"Universalism: objectivity\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Achievement\": 4,\n",
      "    \"Benevolence: caring\": 14,\n",
      "    \"Benevolence: dependability\": 15,\n",
      "    \"Conformity: interpersonal\": 12,\n",
      "    \"Conformity: rules\": 11,\n",
      "    \"Face\": 7,\n",
      "    \"Hedonism\": 3,\n",
      "    \"Humility\": 13,\n",
      "    \"Power: dominance\": 5,\n",
      "    \"Power: resources\": 6,\n",
      "    \"Security: personal\": 8,\n",
      "    \"Security: societal\": 9,\n",
      "    \"Self-direction: action\": 1,\n",
      "    \"Self-direction: thought\": 0,\n",
      "    \"Stimulation\": 2,\n",
      "    \"Tradition\": 10,\n",
      "    \"Universalism: concern\": 16,\n",
      "    \"Universalism: nature\": 17,\n",
      "    \"Universalism: objectivity\": 19,\n",
      "    \"Universalism: tolerance\": 18\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/petasis/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/petasis/.local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4176\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5220\n",
      "  Number of trainable parameters = 109497620\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5220' max='5220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5220/5220 15:02, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>P</th>\n",
       "      <th>R</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.412900</td>\n",
       "      <td>0.364809</td>\n",
       "      <td>0.737511</td>\n",
       "      <td>0.226879</td>\n",
       "      <td>0.347009</td>\n",
       "      <td>0.605087</td>\n",
       "      <td>0.031609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.342200</td>\n",
       "      <td>0.336923</td>\n",
       "      <td>0.713609</td>\n",
       "      <td>0.336966</td>\n",
       "      <td>0.457772</td>\n",
       "      <td>0.654495</td>\n",
       "      <td>0.064176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.301700</td>\n",
       "      <td>0.330019</td>\n",
       "      <td>0.687530</td>\n",
       "      <td>0.398994</td>\n",
       "      <td>0.504950</td>\n",
       "      <td>0.680741</td>\n",
       "      <td>0.074713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.269400</td>\n",
       "      <td>0.324698</td>\n",
       "      <td>0.693897</td>\n",
       "      <td>0.412965</td>\n",
       "      <td>0.517779</td>\n",
       "      <td>0.687639</td>\n",
       "      <td>0.078544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.236900</td>\n",
       "      <td>0.329897</td>\n",
       "      <td>0.662316</td>\n",
       "      <td>0.453758</td>\n",
       "      <td>0.538551</td>\n",
       "      <td>0.702950</td>\n",
       "      <td>0.073755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.213600</td>\n",
       "      <td>0.332376</td>\n",
       "      <td>0.677802</td>\n",
       "      <td>0.430847</td>\n",
       "      <td>0.526819</td>\n",
       "      <td>0.694240</td>\n",
       "      <td>0.080460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.193900</td>\n",
       "      <td>0.337298</td>\n",
       "      <td>0.665973</td>\n",
       "      <td>0.446773</td>\n",
       "      <td>0.534783</td>\n",
       "      <td>0.700209</td>\n",
       "      <td>0.076628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.179900</td>\n",
       "      <td>0.343878</td>\n",
       "      <td>0.644973</td>\n",
       "      <td>0.462420</td>\n",
       "      <td>0.538649</td>\n",
       "      <td>0.704882</td>\n",
       "      <td>0.073755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.166200</td>\n",
       "      <td>0.346158</td>\n",
       "      <td>0.650237</td>\n",
       "      <td>0.460743</td>\n",
       "      <td>0.539330</td>\n",
       "      <td>0.704737</td>\n",
       "      <td>0.073755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.158700</td>\n",
       "      <td>0.346504</td>\n",
       "      <td>0.648562</td>\n",
       "      <td>0.453758</td>\n",
       "      <td>0.533947</td>\n",
       "      <td>0.701447</td>\n",
       "      <td>0.077586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-522\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-522/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-522/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-522/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-522/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-1044\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-1044/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-1044/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-1044/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-1044/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-1566\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-1566/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-1566/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-1566/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-1566/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-2088\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-2088/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-2088/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-2088/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-2088/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-2610\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-2610/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-2610/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-2610/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-2610/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-3132\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-3132/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-3132/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-3132/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-3132/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-3654\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-3654/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-3654/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-3654/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-3654/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-4176\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-4176/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-4176/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-4176/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-4176/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-4698\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-4698/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-4698/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-4698/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-4698/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert-finetuned-sem_eval-english/checkpoint-5220\n",
      "Configuration saved in bert-finetuned-sem_eval-english/checkpoint-5220/config.json\n",
      "Model weights saved in bert-finetuned-sem_eval-english/checkpoint-5220/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-sem_eval-english/checkpoint-5220/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-sem_eval-english/checkpoint-5220/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from bert-finetuned-sem_eval-english/checkpoint-4698 (score: 0.5393295175797219).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='262' max='131' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [131/131 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fold = 1\n",
    "\n",
    "metrics = {\n",
    "    'p': [],\n",
    "    'r': [],\n",
    "    'f1': [],\n",
    "    'roc_auc': [],\n",
    "    'accuracy': []\n",
    "}\n",
    "\n",
    "if n_folds < 2:\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset=encoded_dataset[\"train\"],\n",
    "        eval_dataset=encoded_dataset[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    trainer.train()\n",
    "    results = trainer.evaluate()\n",
    "    for key in metrics:\n",
    "        metrics[key].append(results.get('eval_' + key))\n",
    "else:\n",
    "    for train_idxs, val_idxs in splits:\n",
    "        print(f\"Fold: {fold}\")\n",
    "        ## Important: Don't forget to reload the original model in each fold!\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(labels),\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)\n",
    "        \n",
    "        fold_dataset = DatasetDict({\n",
    "            \"train\":encoded_dataset[\"train\"].select(train_idxs),\n",
    "            \"validation\":encoded_dataset[\"train\"].select(val_idxs),\n",
    "            \"test\":encoded_dataset[\"validation\"]\n",
    "        })\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model,\n",
    "            args,\n",
    "            train_dataset=fold_dataset[\"train\"],\n",
    "            eval_dataset=fold_dataset[\"validation\"],\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "        results = trainer.evaluate()\n",
    "        for key in metrics:\n",
    "            metrics[key].append(results.get('eval_' + key))\n",
    "        fold += 1\n",
    "        # print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "KXmFds8js6P8",
    "outputId": "66ebb2ab-f93f-48aa-a4dc-36f55f2f8559"
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P</th>\n",
       "      <th>R</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.463</td>\n",
       "      <td>0.5462</td>\n",
       "      <td>0.7075</td>\n",
       "      <td>0.0849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>±</th>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.0104</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        P      R      F1  Roc Auc  Accuracy\n",
       "   0.6667  0.463  0.5462   0.7075    0.0849\n",
       "±  0.0180  0.014  0.0104   0.0062    0.0167"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "decimal_digits = 4\n",
    "\n",
    "data = [[round(np.mean(metrics['p']), decimal_digits),\n",
    "         round(np.mean(metrics['r']), decimal_digits),\n",
    "         round(np.mean(metrics['f1']), decimal_digits),\n",
    "         round(np.mean(metrics['roc_auc']), decimal_digits),\n",
    "         round(np.mean(metrics['accuracy']), decimal_digits),\n",
    "        ],\n",
    "        [round(np.std(metrics['p']), decimal_digits),\n",
    "         round(np.std(metrics['r']), decimal_digits),\n",
    "         round(np.std(metrics['f1']), decimal_digits),\n",
    "         round(np.std(metrics['roc_auc']), decimal_digits),\n",
    "         round(np.std(metrics['accuracy']), decimal_digits),\n",
    "        ]]\n",
    "\n",
    "df = pd.DataFrame(data, index=[\"\", \"±\"], columns=[\"P\", \"R\", \"F1\", \"Roc Auc\", \"Accuracy\"])\n",
    "df\n",
    "# df.style.hide_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiloh9eMK91o"
   },
   "source": [
    "## Evaluate\n",
    "\n",
    "After training, we evaluate our model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "cMlebJ83LRYG",
    "outputId": "b18102e7-2198-4beb-c874-d39636f740ed"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.34615832567214966,\n",
       " 'eval_p': 0.6502365930599369,\n",
       " 'eval_r': 0.46074322436434756,\n",
       " 'eval_f1': 0.5393295175797219,\n",
       " 'eval_roc_auc': 0.7047372557865896,\n",
       " 'eval_accuracy': 0.07375478927203065,\n",
       " 'eval_runtime': 5.2357,\n",
       " 'eval_samples_per_second': 199.401,\n",
       " 'eval_steps_per_second': 25.021,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nmvJp0pLq-3"
   },
   "source": [
    "## Inference\n",
    "\n",
    "Let's test the model on a new sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "3fxjfr8PLD42"
   },
   "outputs": [],
   "source": [
    "text = \"I'm happy I can finally train a model for multi-label classification\"\n",
    "\n",
    "encoding = tokenizer(text, return_tensors=\"pt\")\n",
    "encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}\n",
    "\n",
    "outputs = trainer.model(**encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8THm5-XgNHPm"
   },
   "source": [
    "The logits that come out of the model are of shape (batch_size, num_labels). As we are only forwarding a single sentence through the model, the `batch_size` equals 1. The logits is a tensor that contains the (unnormalized) scores for every individual label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KOBosj4UL2tU",
    "outputId": "be370f49-3840-4c76-b193-76083e49701e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = outputs.logits\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DC4XdDaHNVcd"
   },
   "source": [
    "To turn them into actual predicted labels, we first apply a sigmoid function independently to every score, such that every score is turned into a number between 0 and 1, that can be interpreted as a \"probability\" for how certain the model is that a given class belongs to the input text.\n",
    "\n",
    "Next, we use a threshold (typically, 0.5) to turn every probability into either a 1 (which means, we predict the label for the given example) or a 0 (which means, we don't predict the label for the given example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mEkAQleMMT0k",
    "outputId": "fddb51cb-bf01-420a-fd5b-4a7c2e775446"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Achievement']\n"
     ]
    }
   ],
   "source": [
    "# apply sigmoid + threshold\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "probs = sigmoid(logits.squeeze().cpu())\n",
    "predictions = np.zeros(probs.shape)\n",
    "predictions[np.where(probs >= 0.5)] = 1\n",
    "# turn predicted id's into actual label names\n",
    "predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMiblo1Ci0GTlAbbA5wB3mn",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Fine-tuning BERT (and friends) for multi-label text classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "091f8220f33241f288faa0612853585f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6111a73e684a47769bda7183a836ee91",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8b1899a0c4b144d7a5e6599f8afb8b65",
      "value": 3
     }
    },
    "1045bb16e3694410898a73cf1b848917": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bbba60f793c14100934a268063f63d26",
      "placeholder": "​",
      "style": "IPY_MODEL_cd3570ddf67541d7818d97e236c54e54",
      "value": " 3/3 [00:00&lt;00:00, 75.93it/s]"
     }
    },
    "308fa6a7348140ec981a8d6c7d31f346": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5080d322a8034924b652b379c04667ed": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6111a73e684a47769bda7183a836ee91": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b1899a0c4b144d7a5e6599f8afb8b65": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8bc92587e35443488445e7521fbd0a13": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5080d322a8034924b652b379c04667ed",
      "placeholder": "​",
      "style": "IPY_MODEL_cb95e545fbdd4e99903bf634df694c9f",
      "value": "100%"
     }
    },
    "9a1aa9f2cc29473f9f8e5459d2641e76": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8bc92587e35443488445e7521fbd0a13",
       "IPY_MODEL_091f8220f33241f288faa0612853585f",
       "IPY_MODEL_1045bb16e3694410898a73cf1b848917"
      ],
      "layout": "IPY_MODEL_308fa6a7348140ec981a8d6c7d31f346"
     }
    },
    "bbba60f793c14100934a268063f63d26": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb95e545fbdd4e99903bf634df694c9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cd3570ddf67541d7818d97e236c54e54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
